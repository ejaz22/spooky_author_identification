{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Sabeeha\\Desktop\\project\\spooky\\train.csv',index_col='id')\n",
    "test = pd.read_csv(r'C:\\Users\\Sabeeha\\Desktop\\project\\spooky\\test.csv',index_col='id')\n",
    "submission = pd.read_csv(r'C:\\Users\\Sabeeha\\Desktop\\project\\spooky\\sample_submission.csv')\n",
    "#df = pd.concat([train,test], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author\n",
       "id                                                               \n",
       "id26305  This process, however, afforded me no means of...    EAP\n",
       "id17569  It never once occurred to me that the fumbling...    HPL\n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 15663\n",
      "Test size: 3916\n"
     ]
    }
   ],
   "source": [
    " #Split data into train and test\n",
    "train_size = int(len(df) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(df) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['text'][:train_size]\n",
    "Y_train = df['author'][:train_size]\n",
    "\n",
    "X_test = df['text'][train_size:]\n",
    "Y_test = df['author'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenize = Tokenizer(num_words=1000, char_level=False)\n",
    "tokenize.fit_on_texts(list(X_train)+list(X_test)) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(X_train)\n",
    "x_test = tokenize.texts_to_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "y_train = encoder.transform(Y_train)\n",
    "y_test = encoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the label for neural net\n",
    "from keras import utils \n",
    "y_train = utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (15663, 1000)\n",
      "x_test shape: (3916, 1000)\n",
      "y_train shape: (15663, 3)\n",
      "y_test shape: (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(1000,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeding model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "14096/14096 [==============================] - 6s 448us/step - loss: 0.4808 - accuracy: 0.7997 - val_loss: 0.6710 - val_accuracy: 0.7173\n",
      "Epoch 2/10\n",
      "14096/14096 [==============================] - 6s 449us/step - loss: 0.4422 - accuracy: 0.8211 - val_loss: 0.6806 - val_accuracy: 0.7045\n",
      "Epoch 3/10\n",
      "14096/14096 [==============================] - 6s 449us/step - loss: 0.4057 - accuracy: 0.8331 - val_loss: 0.7286 - val_accuracy: 0.7052\n",
      "Epoch 4/10\n",
      "14096/14096 [==============================] - 6s 455us/step - loss: 0.3904 - accuracy: 0.8375 - val_loss: 0.7401 - val_accuracy: 0.7058\n",
      "Epoch 5/10\n",
      "14096/14096 [==============================] - 6s 455us/step - loss: 0.3582 - accuracy: 0.8590 - val_loss: 0.7886 - val_accuracy: 0.7020\n",
      "Epoch 6/10\n",
      "14096/14096 [==============================] - 6s 460us/step - loss: 0.3442 - accuracy: 0.8619 - val_loss: 0.7815 - val_accuracy: 0.6994\n",
      "Epoch 7/10\n",
      "14096/14096 [==============================] - 7s 470us/step - loss: 0.3128 - accuracy: 0.8723 - val_loss: 0.8418 - val_accuracy: 0.7001\n",
      "Epoch 8/10\n",
      "14096/14096 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.88 - 7s 465us/step - loss: 0.3062 - accuracy: 0.8817 - val_loss: 0.8014 - val_accuracy: 0.7141\n",
      "Epoch 9/10\n",
      "14096/14096 [==============================] - 6s 460us/step - loss: 0.3033 - accuracy: 0.8802 - val_loss: 0.8380 - val_accuracy: 0.7020\n",
      "Epoch 10/10\n",
      "14096/14096 [==============================] - 7s 468us/step - loss: 0.2790 - accuracy: 0.8935 - val_loss: 0.8471 - val_accuracy: 0.6943\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3916/3916 [==============================] - 0s 91us/step\n",
      "Test score: 0.8123261775741538\n",
      "Test accuracy: 0.7132278084754944\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=32, \n",
    "                       verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8123261694903681"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log loss\n",
    "from sklearn.metrics import log_loss\n",
    "y_pred = model.predict_proba(x_test)\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    Still, as I urged our leaving Ireland with suc...\n",
      "Name: id02310, dtype: object ...\n",
      "Actual label:EAP\n",
      "Predicted label: MWS\n",
      "\n",
      "text    If a fire wanted fanning, it could readily be ...\n",
      "Name: id24541, dtype: object ...\n",
      "Actual label:EAP\n",
      "Predicted label: EAP\n",
      "\n",
      "text    And when they had broken down the frail door t...\n",
      "Name: id00134, dtype: object ...\n",
      "Actual label:HPL\n",
      "Predicted label: HPL\n",
      "\n",
      "text    While I was thinking how I should possibly man...\n",
      "Name: id27757, dtype: object ...\n",
      "Actual label:EAP\n",
      "Predicted label: MWS\n",
      "\n",
      "text    I am not sure to what limit his knowledge may ...\n",
      "Name: id04081, dtype: object ...\n",
      "Actual label:HPL\n",
      "Predicted label: HPL\n",
      "\n",
      "text    \"The thick and peculiar mist, or smoke, which ...\n",
      "Name: id27337, dtype: object ...\n",
      "Actual label:HPL\n",
      "Predicted label: EAP\n",
      "\n",
      "text    That which is not matter, is not at all unless...\n",
      "Name: id24265, dtype: object ...\n",
      "Actual label:EAP\n",
      "Predicted label: EAP\n",
      "\n",
      "text    I sought for repose although I did not hope fo...\n",
      "Name: id25917, dtype: object ...\n",
      "Actual label:EAP\n",
      "Predicted label: EAP\n",
      "\n",
      "text    Upon the fourth day of the assassination, a pa...\n",
      "Name: id04951, dtype: object ...\n",
      "Actual label:EAP\n",
      "Predicted label: EAP\n",
      "\n",
      "text    \"The tone metaphysical is also a good one.\n",
      "Name: id14549, dtype: object ...\n",
      "Actual label:MWS\n",
      "Predicted label: MWS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ \n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + Y_test.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer\n",
    "\n",
    "Embedding layer creates embedding vectors out of the input words, similarly like word2vec or precalculated glove would do.\n",
    "\n",
    "__input_dim__ : the vocabulary size. This is how many unique words are represented in your corpus.\n",
    "\n",
    "__output_dim__ : the desired dimension of the word vector. For example, if output_dim = 100, then every word will be mapped onto a vector with 100 elements.\n",
    "\n",
    "__input_length__ : the length of your sequences. For example, if your data consists of sentences, then this variable represents how many words there are in a sentence. As disparate sentences typically contain different number of words, it is usually required to pad your sequences such that all sentences are of equal length. The keras.preprocessing.pad_sequence method can be used for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': 1,\n",
       " 'it': 2,\n",
       " 'is': 3,\n",
       " 'the': 4,\n",
       " 'i': 5,\n",
       " 'was': 6,\n",
       " 'like': 7,\n",
       " 'a': 8,\n",
       " 'fantastic': 9,\n",
       " 'horrible': 10,\n",
       " 'acting': 11,\n",
       " 'not': 12,\n",
       " 'this': 13,\n",
       " 'an': 14,\n",
       " 'excellent': 15,\n",
       " 'move': 16,\n",
       " 'you': 17,\n",
       " 'should': 18,\n",
       " 'watch': 19,\n",
       " 'brilliant': 20,\n",
       " 'exceptionally': 21,\n",
       " 'good': 22,\n",
       " 'wonderfully': 23,\n",
       " 'directed': 24,\n",
       " 'and': 25,\n",
       " 'executed': 26,\n",
       " 'its': 27,\n",
       " 'series': 28,\n",
       " 'never': 29,\n",
       " 'watched': 30,\n",
       " 'such': 31,\n",
       " 'brillent': 32,\n",
       " 'wonderful': 33,\n",
       " 'waste': 34,\n",
       " 'of': 35,\n",
       " 'money': 36,\n",
       " 'pathetic': 37,\n",
       " 'picture': 38,\n",
       " 'very': 39,\n",
       " 'boring': 40,\n",
       " 'did': 41,\n",
       " 'will': 42,\n",
       " 'recommend': 43,\n",
       " 'pathe': 44}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#texts = ['This is a text','This is not a text']\n",
    "\n",
    "texts = [\n",
    "   # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathe']\n",
    "\n",
    "\n",
    "#num_words is tne number of unique words in the sequence, if there's more top count words are taken\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define vocabulary size (total number of unique word in corpus)\n",
    "input_dim = len(tokenizer.word_index) + 1\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input length --> maximum length of input documents\n",
    "max_length = max([len(s.split()) for s in texts])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (16, 7)\n",
      "[[0 0 0 0 0 3 1]\n",
      " [0 4 6 9 5 7 2]\n",
      " [0 0 0 0 0 2 3]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 5 7 2]\n",
      " [0 0 0 0 0 8 9]\n",
      " [0 0 0 0 0 8 1]\n",
      " [0 0 0 2 3 8 1]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 6]\n",
      " [0 0 0 5 7 4 1]\n",
      " [0 0 0 0 4 1 6]\n",
      " [0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 4 3]]\n"
     ]
    }
   ],
   "source": [
    "# We add padding to make all the vectors of same length\n",
    "data = pad_sequences(sequences, max_length)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=top_words, output_dim=embedding_vecor_length, input_length=max_length,mask_zero=True))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "output_array = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00634675,  0.03606481,  0.04974747,  0.03115224],\n",
       "        [-0.0340276 , -0.03345232, -0.01004336, -0.02263137]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.01352594, -0.01442224, -0.02554727,  0.04339388],\n",
       "        [ 0.02604628,  0.02247224,  0.01502315, -0.03371266],\n",
       "        [-0.0202619 , -0.03797339, -0.02806621, -0.03407003],\n",
       "        [ 0.03944676,  0.00259546, -0.00703113, -0.01197033],\n",
       "        [-0.03315636, -0.00247096,  0.02489186,  0.00848497],\n",
       "        [-0.0041375 , -0.03498287,  0.02002242, -0.03391895]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.0041375 , -0.03498287,  0.02002242, -0.03391895],\n",
       "        [-0.00634675,  0.03606481,  0.04974747,  0.03115224]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [ 0.03944676,  0.00259546, -0.00703113, -0.01197033],\n",
       "        [-0.03315636, -0.00247096,  0.02489186,  0.00848497],\n",
       "        [-0.0041375 , -0.03498287,  0.02002242, -0.03391895]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [ 0.00940454, -0.03777925,  0.01380876, -0.03837085],\n",
       "        [-0.0202619 , -0.03797339, -0.02806621, -0.03407003]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [ 0.00940454, -0.03777925,  0.01380876, -0.03837085],\n",
       "        [-0.0340276 , -0.03345232, -0.01004336, -0.02263137]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.0041375 , -0.03498287,  0.02002242, -0.03391895],\n",
       "        [-0.00634675,  0.03606481,  0.04974747,  0.03115224],\n",
       "        [ 0.00940454, -0.03777925,  0.01380876, -0.03837085],\n",
       "        [-0.0340276 , -0.03345232, -0.01004336, -0.02263137]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.0041375 , -0.03498287,  0.02002242, -0.03391895],\n",
       "        [ 0.02604628,  0.02247224,  0.01502315, -0.03371266]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [ 0.03944676,  0.00259546, -0.00703113, -0.01197033],\n",
       "        [-0.03315636, -0.00247096,  0.02489186,  0.00848497],\n",
       "        [-0.01352594, -0.01442224, -0.02554727,  0.04339388],\n",
       "        [-0.0340276 , -0.03345232, -0.01004336, -0.02263137]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.01352594, -0.01442224, -0.02554727,  0.04339388],\n",
       "        [-0.0340276 , -0.03345232, -0.01004336, -0.02263137],\n",
       "        [ 0.02604628,  0.02247224,  0.01502315, -0.03371266]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [ 0.03944676,  0.00259546, -0.00703113, -0.01197033]],\n",
       "\n",
       "       [[-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.00111023, -0.01840474,  0.02020403, -0.02580997],\n",
       "        [-0.01352594, -0.01442224, -0.02554727,  0.04339388],\n",
       "        [-0.00634675,  0.03606481,  0.04974747,  0.03115224]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
